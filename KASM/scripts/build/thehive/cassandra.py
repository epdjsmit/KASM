#!/usr/bin/env python3 -tt

def main():
    content = '\n# Cassandra storage config YAML\n\n# NOTE:\n#   See https://cassandra.apache.org/doc/latest/configuration/ for\n#   full explanations of configuration directives\n# /NOTE\n\n# The name of the cluster. This is mainly used to prevent machines in\n# one logical cluster from joining another.\ncluster_name: \'thp\'\n\n# This defines the number of tokens randomly assigned to this node on the ring\n# The more tokens, relative to other nodes, the larger the proportion of data\n# that this node will store. You probably want all nodes to have the same number\n# of tokens assuming they have equal hardware capability.\n#\n# If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\n# and will use the initial_token as described below.\n#\n# Specifying initial_token will override this setting on the node\'s initial start,\n# on subsequent starts, this setting will apply even if initial token is set.\n#\n# See https://cassandra.apache.org/doc/latest/getting_started/production.html#tokens for\n# best practice information about num_tokens.\n#\nnum_tokens: 16\n\n# Triggers automatic allocation of num_tokens tokens for this node. The allocation\n# algorithm attempts to choose tokens in a way that optimizes replicated load over\n# the nodes in the datacenter for the replica factor.\n#\n# The load assigned to each node will be close to proportional to its number of\n# vnodes.\n#\n# Only supported with the Murmur3Partitioner.\n\n# Replica factor is determined via the replication strategy used by the specified\n# keyspace.\n# allocate_tokens_for_keyspace: KEYSPACE\n\n# Replica factor is explicitly set, regardless of keyspace or datacenter.\n# This is the replica factor within the datacenter, like NTS.\nallocate_tokens_for_local_replication_factor: 3\n\n# initial_token allows you to specify tokens manually.  While you can use it with\n# vnodes (num_tokens > 1, above) -- in which case you should provide a \n# comma-separated list -- it\'s primarily used when adding nodes to legacy clusters \n# that do not have vnodes enabled.\n# initial_token:\n\n# May either be "true" or "false" to enable globally\nhinted_handoff_enabled: true\n\n# When hinted_handoff_enabled is true, a black list of data centers that will not\n# perform hinted handoff\n# hinted_handoff_disabled_datacenters:\n#    - DC1\n#    - DC2\n\n# this defines the maximum amount of time a dead host will have hints\n# generated.  After it has been dead this long, new hints for it will not be\n# created until it has been seen alive and gone down again.\nmax_hint_window_in_ms: 10800000 # 3 hours\n\n# Maximum throttle in KBs per second, per delivery thread.  This will be\n# reduced proportionally to the number of nodes in the cluster.  (If there\n# are two nodes in the cluster, each delivery thread will use the maximum\n# rate; if there are three, each will throttle to half of the maximum,\n# since we expect two nodes to be delivering hints simultaneously.)\nhinted_handoff_throttle_in_kb: 1024\n\n# Number of threads with which to deliver hints;\n# Consider increasing this number when you have multi-dc deployments, since\n# cross-dc handoff tends to be slower\nmax_hints_delivery_threads: 2\n\n# Directory where Cassandra should store hints.\n# If not set, the default directory is $CASSANDRA_HOME/data/hints.\nhints_directory: /var/lib/cassandra/hints\n\n# How often hints should be flushed from the internal buffers to disk.\n# Will *not* trigger fsync.\nhints_flush_period_in_ms: 10000\n\n# Maximum size for a single hints file, in megabytes.\nmax_hints_file_size_in_mb: 128\n\n# Compression to apply to the hint files. If omitted, hints files\n# will be written uncompressed. LZ4, Snappy, and Deflate compressors\n# are supported.\n#hints_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Maximum throttle in KBs per second, total. This will be\n# reduced proportionally to the number of nodes in the cluster.\nbatchlog_replay_throttle_in_kb: 1024\n\n# Authentication backend, implementing IAuthenticator; used to identify users\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\n# PasswordAuthenticator}.\n#\n# - AllowAllAuthenticator performs no checks - set it to disable authentication.\n# - PasswordAuthenticator relies on username/password pairs to authenticate\n#   users. It keeps usernames and hashed passwords in system_auth.roles table.\n#   Please increase system_auth keyspace replication factor if you use this authenticator.\n#   If using PasswordAuthenticator, CassandraRoleManager must also be used (see below)\nauthenticator: AllowAllAuthenticator\n\n# Authorization backend, implementing IAuthorizer; used to limit access/provide permissions\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthorizer,\n# CassandraAuthorizer}.\n#\n# - AllowAllAuthorizer allows any action to any user - set it to disable authorization.\n# - CassandraAuthorizer stores permissions in system_auth.role_permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nauthorizer: AllowAllAuthorizer\n\n# Part of the Authentication & Authorization backend, implementing IRoleManager; used\n# to maintain grants and memberships between roles.\n# Out of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\n# which stores role information in the system_auth keyspace. Most functions of the\n# IRoleManager require an authenticated login, so unless the configured IAuthenticator\n# actually implements authentication, most of this functionality will be unavailable.\n#\n# - CassandraRoleManager stores role data in the system_auth keyspace. Please\n#   increase system_auth keyspace replication factor if you use this role manager.\nrole_manager: CassandraRoleManager\n\n# Network authorization backend, implementing INetworkAuthorizer; used to restrict user\n# access to certain DCs\n# Out of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllNetworkAuthorizer,\n# CassandraNetworkAuthorizer}.\n#\n# - AllowAllNetworkAuthorizer allows access to any DC to any user - set it to disable authorization.\n# - CassandraNetworkAuthorizer stores permissions in system_auth.network_permissions table. Please\n#   increase system_auth keyspace replication factor if you use this authorizer.\nnetwork_authorizer: AllowAllNetworkAuthorizer\n\n# Validity period for roles cache (fetching granted roles can be an expensive\n# operation depending on the role manager, CassandraRoleManager is one example)\n# Granted roles are cached for authenticated sessions in AuthenticatedUser and\n# after the period specified here, become eligible for (async) reload.\n# Defaults to 2000, set to 0 to disable caching entirely.\n# Will be disabled automatically for AllowAllAuthenticator.\nroles_validity_in_ms: 2000\n\n# Refresh interval for roles cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If roles_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as roles_validity_in_ms.\n# roles_update_interval_in_ms: 2000\n\n# Validity period for permissions cache (fetching permissions can be an\n# expensive operation depending on the authorizer, CassandraAuthorizer is\n# one example). Defaults to 2000, set to 0 to disable.\n# Will be disabled automatically for AllowAllAuthorizer.\npermissions_validity_in_ms: 2000\n\n# Refresh interval for permissions cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If permissions_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as permissions_validity_in_ms.\n# permissions_update_interval_in_ms: 2000\n\n# Validity period for credentials cache. This cache is tightly coupled to\n# the provided PasswordAuthenticator implementation of IAuthenticator. If\n# another IAuthenticator implementation is configured, this cache will not\n# be automatically used and so the following settings will have no effect.\n# Please note, credentials are cached in their encrypted form, so while\n# activating this cache may reduce the number of queries made to the\n# underlying table, it may not  bring a significant reduction in the\n# latency of individual authentication attempts.\n# Defaults to 2000, set to 0 to disable credentials caching.\ncredentials_validity_in_ms: 2000\n\n# Refresh interval for credentials cache (if enabled).\n# After this interval, cache entries become eligible for refresh. Upon next\n# access, an async reload is scheduled and the old value returned until it\n# completes. If credentials_validity_in_ms is non-zero, then this must be\n# also.\n# Defaults to the same value as credentials_validity_in_ms.\n# credentials_update_interval_in_ms: 2000\n\n# The partitioner is responsible for distributing groups of rows (by\n# partition key) across nodes in the cluster. The partitioner can NOT be\n# changed without reloading all data.  If you are adding nodes or upgrading,\n# you should set this to the same partitioner that you are currently using.\n#\n# The default partitioner is the Murmur3Partitioner. Older partitioners\n# such as the RandomPartitioner, ByteOrderedPartitioner, and\n# OrderPreservingPartitioner have been included for backward compatibility only.\n# For new clusters, you should NOT change this value.\n#\npartitioner: org.apache.cassandra.dht.Murmur3Partitioner\n\n# Directories where Cassandra should store data on disk. If multiple\n# directories are specified, Cassandra will spread data evenly across \n# them by partitioning the token ranges.\n# If not set, the default directory is $CASSANDRA_HOME/data/data.\ndata_file_directories:\n    - /var/lib/cassandra/data\n\n# Directory were Cassandra should store the data of the local system keyspaces.\n# By default Cassandra will store the data of the local system keyspaces in the first of the data directories specified\n# by data_file_directories.\n# This approach ensures that if one of the other disks is lost Cassandra can continue to operate. For extra security\n# this setting allows to store those data on a different directory that provides redundancy.\n# local_system_data_file_directory:\n\n# commit log.  when running on magnetic HDD, this should be a\n# separate spindle than the data directories.\n# If not set, the default directory is $CASSANDRA_HOME/data/commitlog.\ncommitlog_directory: /var/lib/cassandra/commitlog\n\n# Enable / disable CDC functionality on a per-node basis. This modifies the logic used\n# for write path allocation rejection (standard: never reject. cdc: reject Mutation\n# containing a CDC-enabled table if at space limit in cdc_raw_directory).\ncdc_enabled: false\n\n# CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the\n# segment contains mutations for a CDC-enabled table. This should be placed on a\n# separate spindle than the data directories. If not set, the default directory is\n# $CASSANDRA_HOME/data/cdc_raw.\n# cdc_raw_directory: /var/lib/cassandra/cdc_raw\n\n# Policy for data disk failures:\n#\n# die\n#   shut down gossip and client transports and kill the JVM for any fs errors or\n#   single-sstable errors, so the node can be replaced.\n#\n# stop_paranoid\n#   shut down gossip and client transports even for single-sstable errors,\n#   kill the JVM for errors during startup.\n#\n# stop\n#   shut down gossip and client transports, leaving the node effectively dead, but\n#   can still be inspected via JMX, kill the JVM for errors during startup.\n#\n# best_effort\n#    stop using the failed disk and respond to requests based on\n#    remaining available sstables.  This means you WILL see obsolete\n#    data at CL.ONE!\n#\n# ignore\n#    ignore fatal errors and let requests fail, as in pre-1.2 Cassandra\ndisk_failure_policy: stop\n\n# Policy for commit disk failures:\n#\n# die\n#   shut down the node and kill the JVM, so the node can be replaced.\n#\n# stop\n#   shut down the node, leaving the node effectively dead, but\n#   can still be inspected via JMX.\n#\n# stop_commit\n#   shutdown the commit log, letting writes collect but\n#   continuing to service reads, as in pre-2.0.5 Cassandra\n#\n# ignore\n#   ignore fatal errors and let the batches fail\ncommit_failure_policy: stop\n\n# Maximum size of the native protocol prepared statement cache\n#\n# Valid values are either "auto" (omitting the value) or a value greater 0.\n#\n# Note that specifying a too large value will result in long running GCs and possbily\n# out-of-memory errors. Keep the value at a small fraction of the heap.\n#\n# If you constantly see "prepared statements discarded in the last minute because\n# cache limit reached" messages, the first step is to investigate the root cause\n# of these messages and check whether prepared statements are used correctly -\n# i.e. use bind markers for variable parts.\n#\n# Do only change the default value, if you really have more prepared statements than\n# fit in the cache. In most cases it is not neccessary to change this value.\n# Constantly re-preparing statements is a performance penalty.\n#\n# Default value ("auto") is 1/256th of the heap or 10MB, whichever is greater\nprepared_statements_cache_size_mb:\n\n# Maximum size of the key cache in memory.\n#\n# Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\n# minimum, sometimes more. The key cache is fairly tiny for the amount of\n# time it saves, so it\'s worthwhile to use it at large numbers.\n# The row cache saves even more time, but must contain the entire row,\n# so it is extremely space-intensive. It\'s best to only use the\n# row cache if you have hot rows or static rows.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it "auto" (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.\nkey_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the key cache. Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 14400 or 4 hours.\nkey_cache_save_period: 14400\n\n# Number of keys from the key cache to save\n# Disabled by default, meaning all keys are going to be saved\n# key_cache_keys_to_save: 100\n\n# Row cache implementation class name. Available implementations:\n#\n# org.apache.cassandra.cache.OHCProvider\n#   Fully off-heap row cache implementation (default).\n#\n# org.apache.cassandra.cache.SerializingCacheProvider\n#   This is the row cache implementation availabile\n#   in previous releases of Cassandra.\n# row_cache_class_name: org.apache.cassandra.cache.OHCProvider\n\n# Maximum size of the row cache in memory.\n# Please note that OHC cache implementation requires some additional off-heap memory to manage\n# the map structures and some in-flight memory during operations before/after cache entries can be\n# accounted against the cache capacity. This overhead is usually small compared to the whole capacity.\n# Do not specify more memory that the system can afford in the worst usual situation and leave some\n# headroom for OS block level cache. Do never allow your system to swap.\n#\n# Default value is 0, to disable row caching.\nrow_cache_size_in_mb: 0\n\n# Duration in seconds after which Cassandra should save the row cache.\n# Caches are saved to saved_caches_directory as specified in this configuration file.\n#\n# Saved caches greatly improve cold-start speeds, and is relatively cheap in\n# terms of I/O for the key cache. Row cache saving is much more expensive and\n# has limited use.\n#\n# Default is 0 to disable saving the row cache.\nrow_cache_save_period: 0\n\n# Number of keys from the row cache to save.\n# Specify 0 (which is the default), meaning all keys are going to be saved\n# row_cache_keys_to_save: 100\n\n# Maximum size of the counter cache in memory.\n#\n# Counter cache helps to reduce counter locks\' contention for hot counter cells.\n# In case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\n# write entirely. With RF > 1 a counter cache hit will still help to reduce the duration\n# of the lock hold, helping with hot counter cell updates, but will not allow skipping\n# the read entirely. Only the local (clock, count) tuple of a counter cell is kept\n# in memory, not the whole counter, so it\'s relatively cheap.\n#\n# NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.\n#\n# Default value is empty to make it "auto" (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\n# NOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.\ncounter_cache_size_in_mb:\n\n# Duration in seconds after which Cassandra should\n# save the counter cache (keys only). Caches are saved to saved_caches_directory as\n# specified in this configuration file.\n#\n# Default is 7200 or 2 hours.\ncounter_cache_save_period: 7200\n\n# Number of keys from the counter cache to save\n# Disabled by default, meaning all keys are going to be saved\n# counter_cache_keys_to_save: 100\n\n# saved caches\n# If not set, the default directory is $CASSANDRA_HOME/data/saved_caches.\nsaved_caches_directory: /var/lib/cassandra/saved_caches\n\n# Number of seconds the server will wait for each cache (row, key, etc ...) to load while starting\n# the Cassandra process. Setting this to a negative value is equivalent to disabling all cache loading on startup\n# while still having the cache during runtime.\n# cache_load_timeout_seconds: 30\n\n# commitlog_sync may be either "periodic", "group", or "batch." \n# \n# When in batch mode, Cassandra won\'t ack writes until the commit log\n# has been flushed to disk.  Each incoming write will trigger the flush task.\n# commitlog_sync_batch_window_in_ms is a deprecated value. Previously it had\n# almost no value, and is being removed.\n#\n# commitlog_sync_batch_window_in_ms: 2\n#\n# group mode is similar to batch mode, where Cassandra will not ack writes\n# until the commit log has been flushed to disk. The difference is group\n# mode will wait up to commitlog_sync_group_window_in_ms between flushes.\n#\n# commitlog_sync_group_window_in_ms: 1000\n#\n# the default option is "periodic" where writes may be acked immediately\n# and the CommitLog is simply synced every commitlog_sync_period_in_ms\n# milliseconds.\ncommitlog_sync: periodic\ncommitlog_sync_period_in_ms: 10000\n\n# When in periodic commitlog mode, the number of milliseconds to block writes\n# while waiting for a slow disk flush to complete.\n# periodic_commitlog_sync_lag_block_in_ms: \n\n# The size of the individual commitlog file segments.  A commitlog\n# segment may be archived, deleted, or recycled once all the data\n# in it (potentially from each columnfamily in the system) has been\n# flushed to sstables.\n#\n# The default size is 32, which is almost always fine, but if you are\n# archiving commitlog segments (see commitlog_archiving.properties),\n# then you probably want a finer granularity of archiving; 8 or 16 MB\n# is reasonable.\n# Max mutation size is also configurable via max_mutation_size_in_kb setting in\n# cassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.\n# This should be positive and less than 2048.\n#\n# NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\n# be set to at least twice the size of max_mutation_size_in_kb / 1024\n#\ncommitlog_segment_size_in_mb: 32\n\n# Compression to apply to the commit log. If omitted, the commit log\n# will be written uncompressed.  LZ4, Snappy, and Deflate compressors\n# are supported.\n# commitlog_compression:\n#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n\n# Compression to apply to SSTables as they flush for compressed tables.\n# Note that tables without compression enabled do not respect this flag.\n#\n# As high ratio compressors like LZ4HC, Zstd, and Deflate can potentially\n# block flushes for too long, the default is to flush with a known fast\n# compressor in those cases. Options are:\n#\n# none : Flush without compressing blocks but while still doing checksums.\n# fast : Flush with a fast compressor. If the table is already using a\n#        fast compressor that compressor is used.\n# table: Always flush with the same compressor that the table uses. This\n#        was the pre 4.0 behavior.\n#\n# flush_compression: fast\n\n# any class that implements the SeedProvider interface and has a\n# constructor that takes a Map<String, String> of parameters will do.\nseed_provider:\n    # Addresses of hosts that are deemed contact points. \n    # Cassandra nodes use this list of hosts to find each other and learn\n    # the topology of the ring.  You must change this if you are running\n    # multiple nodes!\n    - class_name: org.apache.cassandra.locator.SimpleSeedProvider\n      parameters:\n          # seeds is actually a comma-delimited list of addresses.\n          # Ex: "<ip1>,<ip2>,<ip3>"\n          - seeds: "127.0.0.1:7000"\n\n# For workloads with more data than can fit in memory, Cassandra\'s\n# bottleneck will be reads that need to fetch data from\n# disk. "concurrent_reads" should be set to (16 * number_of_drives) in\n# order to allow the operations to enqueue low enough in the stack\n# that the OS and drives can reorder them. Same applies to\n# "concurrent_counter_writes", since counter writes read the current\n# values before incrementing and writing them back.\n#\n# On the other hand, since writes are almost never IO bound, the ideal\n# number of "concurrent_writes" is dependent on the number of cores in\n# your system; (8 * number_of_cores) is a good rule of thumb.\nconcurrent_reads: 32\nconcurrent_writes: 32\nconcurrent_counter_writes: 32\n\n# For materialized view writes, as there is a read involved, so this should\n# be limited by the less of concurrent reads or concurrent writes.\nconcurrent_materialized_view_writes: 32\n\n# Maximum memory to use for inter-node and client-server networking buffers.\n#\n# Defaults to the smaller of 1/16 of heap or 128MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# networking_cache_size_in_mb: 128\n\n# Enable the sstable chunk cache.  The chunk cache will store recently accessed\n# sections of the sstable in-memory as uncompressed buffers.\n# file_cache_enabled: false\n\n# Maximum memory to use for sstable chunk cache and buffer pooling.\n# 32MB of this are reserved for pooling buffers, the rest is used for chunk cache\n# that holds uncompressed sstable chunks.\n# Defaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\n# so is in addition to the memory allocated for heap. The cache also has on-heap\n# overhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\n# if the default 64k chunk size is used).\n# Memory is only allocated when needed.\n# file_cache_size_in_mb: 512\n\n# Flag indicating whether to allocate on or off heap when the sstable buffer\n# pool is exhausted, that is when it has exceeded the maximum memory\n# file_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.\n\n# buffer_pool_use_heap_if_exhausted: true\n\n# The strategy for optimizing disk read\n# Possible values are:\n# ssd (for solid state disks, the default)\n# spinning (for spinning disks)\n# disk_optimization_strategy: ssd\n\n# Total permitted memory to use for memtables. Cassandra will stop\n# accepting writes when the limit is exceeded until a flush completes,\n# and will trigger a flush based on memtable_cleanup_threshold\n# If omitted, Cassandra will set both to 1/4 the size of the heap.\n# memtable_heap_space_in_mb: 2048\n# memtable_offheap_space_in_mb: 2048\n\n# memtable_cleanup_threshold is deprecated. The default calculation\n# is the only reasonable choice. See the comments on  memtable_flush_writers\n# for more information.\n#\n# Ratio of occupied non-flushing memtable size to total permitted size\n# that will trigger a flush of the largest memtable. Larger mct will\n# mean larger flushes and hence less compaction, but also less concurrent\n# flush activity which can make it difficult to keep your disks fed\n# under heavy write load.\n#\n# memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)\n# memtable_cleanup_threshold: 0.11\n\n# Specify the way Cassandra allocates and manages memtable memory.\n# Options are:\n#\n# heap_buffers\n#   on heap nio buffers\n#\n# offheap_buffers\n#   off heap (direct) nio buffers\n#\n# offheap_objects\n#    off heap objects\nmemtable_allocation_type: heap_buffers\n\n# Limit memory usage for Merkle tree calculations during repairs. The default\n# is 1/16th of the available heap. The main tradeoff is that smaller trees\n# have less resolution, which can lead to over-streaming data. If you see heap\n# pressure during repairs, consider lowering this, but you cannot go below\n# one megabyte. If you see lots of over-streaming, consider raising\n# this or using subrange repair.\n#\n# For more details see https://issues.apache.org/jira/browse/CASSANDRA-14096.\n#\n# repair_session_space_in_mb:\n\n# Total space to use for commit logs on disk.\n#\n# If space gets above this value, Cassandra will flush every dirty CF\n# in the oldest segment and remove it.  So a small total commitlog space\n# will tend to cause more flush activity on less-active columnfamilies.\n#\n# The default value is the smaller of 8192, and 1/4 of the total space\n# of the commitlog volume.\n#\n# commitlog_total_space_in_mb: 8192\n\n# This sets the number of memtable flush writer threads per disk\n# as well as the total number of memtables that can be flushed concurrently.\n# These are generally a combination of compute and IO bound.\n#\n# Memtable flushing is more CPU efficient than memtable ingest and a single thread\n# can keep up with the ingest rate of a whole server on a single fast disk\n# until it temporarily becomes IO bound under contention typically with compaction.\n# At that point you need multiple flush threads. At some point in the future\n# it may become CPU bound all the time.\n#\n# You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation\n# metric which should be 0, but will be non-zero if threads are blocked waiting on flushing\n# to free memory.\n#\n# memtable_flush_writers defaults to two for a single data directory.\n# This means that two  memtables can be flushed concurrently to the single data directory.\n# If you have multiple data directories the default is one memtable flushing at a time\n# but the flush will use a thread per data directory so you will get two or more writers.\n#\n# Two is generally enough to flush on a fast disk [array] mounted as a single data directory.\n# Adding more flush writers will result in smaller more frequent flushes that introduce more\n# compaction overhead.\n#\n# There is a direct tradeoff between number of memtables that can be flushed concurrently\n# and flush size and frequency. More is not better you just need enough flush writers\n# to never stall waiting for flushing to free memory.\n#\n#memtable_flush_writers: 2\n\n# Total space to use for change-data-capture logs on disk.\n#\n# If space gets above this value, Cassandra will throw WriteTimeoutException\n# on Mutations including tables with CDC enabled. A CDCCompactor is responsible\n# for parsing the raw CDC logs and deleting them when parsing is completed.\n#\n# The default value is the min of 4096 mb and 1/8th of the total space\n# of the drive where cdc_raw_directory resides.\n# cdc_total_space_in_mb: 4096\n\n# When we hit our cdc_raw limit and the CDCCompactor is either running behind\n# or experiencing backpressure, we check at the following interval to see if any\n# new space for cdc-tracked tables has been made available. Default to 250ms\n# cdc_free_space_check_interval_ms: 250\n\n# A fixed memory pool size in MB for for SSTable index summaries. If left\n# empty, this will default to 5% of the heap size. If the memory usage of\n# all index summaries exceeds this limit, SSTables with low read rates will\n# shrink their index summaries in order to meet this limit.  However, this\n# is a best-effort process. In extreme conditions Cassandra may need to use\n# more than this amount of memory.\nindex_summary_capacity_in_mb:\n\n# How frequently index summaries should be resampled.  This is done\n# periodically to redistribute memory from the fixed-size pool to sstables\n# proportional their recent read rates.  Setting to -1 will disable this\n# process, leaving existing index summaries at their current sampling level.\nindex_summary_resize_interval_in_minutes: 60\n\n# Whether to, when doing sequential writing, fsync() at intervals in\n# order to force the operating system to flush the dirty\n# buffers. Enable this to avoid sudden dirty buffer flushing from\n# impacting read latencies. Almost always a good idea on SSDs; not\n# necessarily on platters.\ntrickle_fsync: false\ntrickle_fsync_interval_in_kb: 10240\n\n# TCP port, for commands and data\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nstorage_port: 7000\n\n# SSL port, for legacy encrypted communication. This property is unused unless enabled in\n# server_encryption_options (see below). As of cassandra 4.0, this property is deprecated\n# as a single port can be used for either/both secure and insecure connections.\n# For security reasons, you should not expose this port to the internet. Firewall it if needed.\nssl_storage_port: 7001\n\n# Address or interface to bind to and tell other Cassandra nodes to connect to.\n# You _must_ change this if you want multiple nodes to be able to communicate!\n#\n# Set listen_address OR listen_interface, not both.\n#\n# Leaving it blank leaves it up to InetAddress.getLocalHost(). This\n# will always do the Right Thing _if_ the node is properly configured\n# (hostname, name resolution, etc), and the Right Thing is to use the\n# address associated with the hostname (it might not be). If unresolvable\n# it will fall back to InetAddress.getLoopbackAddress(), which is wrong for production systems.\n#\n# Setting listen_address to 0.0.0.0 is always wrong.\n#\nlisten_address: localhost\n\n# Set listen_address OR listen_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n# listen_interface: eth0\n\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\n# listen_interface_prefer_ipv6: false\n\n# Address to broadcast to other Cassandra nodes\n# Leaving this blank will set it to the same value as listen_address\n# broadcast_address: 1.2.3.4\n\n# When using multiple physical network interfaces, set this\n# to true to listen on broadcast_address in addition to\n# the listen_address, allowing nodes to communicate in both\n# interfaces.\n# Ignore this property if the network configuration automatically\n# routes  between the public and private networks such as EC2.\n# listen_on_broadcast_address: false\n\n# Internode authentication backend, implementing IInternodeAuthenticator;\n# used to allow/disallow connections from peer nodes.\n# internode_authenticator: org.apache.cassandra.auth.AllowAllInternodeAuthenticator\n\n# Whether to start the native transport server.\n# The address on which the native transport is bound is defined by rpc_address.\nstart_native_transport: true\n# port for the CQL native transport to listen for clients on\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nnative_transport_port: 9042\n# Enabling native transport encryption in client_encryption_options allows you to either use\n# encryption for the standard port or to use a dedicated, additional port along with the unencrypted\n# standard native_transport_port.\n# Enabling client encryption and keeping native_transport_port_ssl disabled will use encryption\n# for native_transport_port. Setting native_transport_port_ssl to a different value\n# from native_transport_port will use encryption for native_transport_port_ssl while\n# keeping native_transport_port unencrypted.\n# native_transport_port_ssl: 9142\n# The maximum threads for handling requests (note that idle threads are stopped\n# after 30 seconds so there is not corresponding minimum setting).\n# native_transport_max_threads: 128\n#\n# The maximum size of allowed frame. Frame (requests) larger than this will\n# be rejected as invalid. The default is 256MB. If you\'re changing this parameter,\n# you may want to adjust max_value_size_in_mb accordingly. This should be positive and less than 2048.\n# native_transport_max_frame_size_in_mb: 256\n\n# The maximum number of concurrent client connections.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections: -1\n\n# The maximum number of concurrent client connections per source ip.\n# The default is -1, which means unlimited.\n# native_transport_max_concurrent_connections_per_ip: -1\n\n# Controls whether Cassandra honors older, yet currently supported, protocol versions.\n# The default is true, which means all supported protocols will be honored.\nnative_transport_allow_older_protocols: true\n\n# Controls when idle client connections are closed. Idle connections are ones that had neither reads\n# nor writes for a time period.\n#\n# Clients may implement heartbeats by sending OPTIONS native protocol message after a timeout, which\n# will reset idle timeout timer on the server side. To close idle client connections, corresponding\n# values for heartbeat intervals have to be set on the client side.\n#\n# Idle connection timeouts are disabled by default.\n# native_transport_idle_timeout_in_ms: 60000\n\n# The address or interface to bind the native transport server to.\n#\n# Set rpc_address OR rpc_interface, not both.\n#\n# Leaving rpc_address blank has the same effect as on listen_address\n# (i.e. it will be based on the configured hostname of the node).\n#\n# Note that unlike listen_address, you can specify 0.0.0.0, but you must also\n# set broadcast_rpc_address to a value other than 0.0.0.0.\n#\n# For security reasons, you should not expose this port to the internet.  Firewall it if needed.\nrpc_address: localhost\n\n# Set rpc_address OR rpc_interface, not both. Interfaces must correspond\n# to a single address, IP aliasing is not supported.\n# rpc_interface: eth1\n\n# If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\n# you can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\n# address will be used. If true the first ipv6 address will be used. Defaults to false preferring\n# ipv4. If there is only one address it will be selected regardless of ipv4/ipv6.\n# rpc_interface_prefer_ipv6: false\n\n# RPC address to broadcast to drivers and other Cassandra nodes. This cannot\n# be set to 0.0.0.0. If left blank, this will be set to the value of\n# rpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\n# be set.\n# broadcast_rpc_address: 1.2.3.4\n\n# enable or disable keepalive on rpc/native connections\nrpc_keepalive: true\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# See also:\n# /proc/sys/net/core/wmem_max\n# /proc/sys/net/core/rmem_max\n# /proc/sys/net/ipv4/tcp_wmem\n# /proc/sys/net/ipv4/tcp_wmem\n# and \'man tcp\'\n# internode_socket_send_buffer_size_in_bytes:\n\n# Uncomment to set socket buffer size for internode communication\n# Note that when setting this, the buffer size is limited by net.core.wmem_max\n# and when not setting it it is defined by net.ipv4.tcp_wmem\n# internode_socket_receive_buffer_size_in_bytes:\n\n# Set to true to have Cassandra create a hard link to each sstable\n# flushed or streamed locally in a backups/ subdirectory of the\n# keyspace data.  Removing these links is the operator\'s\n# responsibility.\nincremental_backups: false\n\n# Whether or not to take a snapshot before each compaction.  Be\n# careful using this option, since Cassandra won\'t clean up the\n# snapshots for you.  Mostly useful if you\'re paranoid when there\n# is a data format change.\nsnapshot_before_compaction: false\n\n# Whether or not a snapshot is taken of the data before keyspace truncation\n# or dropping of column families. The STRONGLY advised default of true \n# should be used to provide data safety. If you set this flag to false, you will\n# lose data on truncation or drop.\nauto_snapshot: true\n\n# The act of creating or clearing a snapshot involves creating or removing\n# potentially tens of thousands of links, which can cause significant performance\n# impact, especially on consumer grade SSDs. A non-zero value here can\n# be used to throttle these links to avoid negative performance impact of\n# taking and clearing snapshots\nsnapshot_links_per_second: 0\n\n# Granularity of the collation index of rows within a partition.\n# Increase if your rows are large, or if you have a very large\n# number of rows per partition.  The competing goals are these:\n#\n# - a smaller granularity means more index entries are generated\n#   and looking up rows withing the partition by collation column\n#   is faster\n# - but, Cassandra will keep the collation index in memory for hot\n#   rows (as part of the key cache), so a larger granularity means\n#   you can cache more hot rows\ncolumn_index_size_in_kb: 64\n\n# Per sstable indexed key cache entries (the collation index in memory\n# mentioned above) exceeding this size will not be held on heap.\n# This means that only partition information is held on heap and the\n# index entries are read from disk.\n#\n# Note that this size refers to the size of the\n# serialized index information and not the size of the partition.\ncolumn_index_cache_size_in_kb: 2\n\n# Number of simultaneous compactions to allow, NOT including\n# validation "compactions" for anti-entropy repair.  Simultaneous\n# compactions can help preserve read performance in a mixed read/write\n# workload, by mitigating the tendency of small sstables to accumulate\n# during a single long running compactions. The default is usually\n# fine and if you experience problems with compaction running too\n# slowly or too fast, you should look at\n# compaction_throughput_mb_per_sec first.\n#\n# concurrent_compactors defaults to the smaller of (number of disks,\n# number of cores), with a minimum of 2 and a maximum of 8.\n# \n# If your data directories are backed by SSD, you should increase this\n# to the number of cores.\n#concurrent_compactors: 1\n\n# Number of simultaneous repair validations to allow. If not set or set to\n# a value less than 1, it defaults to the value of concurrent_compactors.\n# To set a value greeater than concurrent_compactors at startup, the system\n# property cassandra.allow_unlimited_concurrent_validations must be set to\n# true. To dynamically resize to a value > concurrent_compactors on a running\n# node, first call the bypassConcurrentValidatorsLimit method on the\n# org.apache.cassandra.db:type=StorageService mbean\n# concurrent_validations: 0\n\n# Number of simultaneous materialized view builder tasks to allow.\nconcurrent_materialized_view_builders: 1\n\n# Throttles compaction to the given total throughput across the entire\n# system. The faster you insert data, the faster you need to compact in\n# order to keep the sstable count down, but in general, setting this to\n# 16 to 32 times the rate you are inserting data is more than sufficient.\n# Setting this to 0 disables throttling. Note that this accounts for all types\n# of compaction, including validation compaction (building Merkle trees\n# for repairs).\ncompaction_throughput_mb_per_sec: 64\n\n# When compacting, the replacement sstable(s) can be opened before they\n# are completely written, and used in place of the prior sstables for\n# any range that has been written. This helps to smoothly transfer reads \n# between the sstables, reducing page cache churn and keeping hot rows hot\nsstable_preemptive_open_interval_in_mb: 50\n\n# When enabled, permits Cassandra to zero-copy stream entire eligible\n# SSTables between nodes, including every component.\n# This speeds up the network transfer significantly subject to\n# throttling specified by stream_throughput_outbound_megabits_per_sec.\n# Enabling this will reduce the GC pressure on sending and receiving node.\n# When unset, the default is enabled. While this feature tries to keep the\n# disks balanced, it cannot guarantee it. This feature will be automatically\n# disabled if internode encryption is enabled.\n# stream_entire_sstables: true\n\n# Throttles all outbound streaming file transfers on this node to the\n# given total throughput in Mbps. This is necessary because Cassandra does\n# mostly sequential IO when streaming data during bootstrap or repair, which\n# can lead to saturating the network connection and degrading rpc performance.\n# When unset, the default is 200 Mbps or 25 MB/s.\n# stream_throughput_outbound_megabits_per_sec: 200\n\n# Throttles all streaming file transfer between the datacenters,\n# this setting allows users to throttle inter dc stream throughput in addition\n# to throttling all network stream traffic as configured with\n# stream_throughput_outbound_megabits_per_sec\n# When unset, the default is 200 Mbps or 25 MB/s\n# inter_dc_stream_throughput_outbound_megabits_per_sec: 200\n\n# Server side timeouts for requests. The server will return a timeout exception\n# to the client if it can\'t complete an operation within the corresponding\n# timeout. Those settings are a protection against:\n#   1) having client wait on an operation that might never terminate due to some\n#      failures.\n#   2) operations that use too much CPU/read too much data (leading to memory build\n#      up) by putting a limit to how long an operation will execute.\n# For this reason, you should avoid putting these settings too high. In other words, \n# if you are timing out requests because of underlying resource constraints then \n# increasing the timeout will just cause more problems. Of course putting them too \n# low is equally ill-advised since clients could get timeouts even for successful \n# operations just because the timeout setting is too tight.\n\n# How long the coordinator should wait for read operations to complete.\n# Lowest acceptable value is 10 ms.\nread_request_timeout_in_ms: 5000\n# How long the coordinator should wait for seq or index scans to complete.\n# Lowest acceptable value is 10 ms.\nrange_request_timeout_in_ms: 10000\n# How long the coordinator should wait for writes to complete.\n# Lowest acceptable value is 10 ms.\nwrite_request_timeout_in_ms: 2000\n# How long the coordinator should wait for counter writes to complete.\n# Lowest acceptable value is 10 ms.\ncounter_write_request_timeout_in_ms: 5000\n# How long a coordinator should continue to retry a CAS operation\n# that contends with other proposals for the same row.\n# Lowest acceptable value is 10 ms.\ncas_contention_timeout_in_ms: 1000\n# How long the coordinator should wait for truncates to complete\n# (This can be much longer, because unless auto_snapshot is disabled\n# we need to flush first so we can snapshot before removing the data.)\n# Lowest acceptable value is 10 ms.\ntruncate_request_timeout_in_ms: 60000\n# The default timeout for other, miscellaneous operations.\n# Lowest acceptable value is 10 ms.\nrequest_timeout_in_ms: 10000\n\n# Defensive settings for protecting Cassandra from true network partitions.\n# See (CASSANDRA-14358) for details.\n#\n# The amount of time to wait for internode tcp connections to establish.\n# internode_tcp_connect_timeout_in_ms: 2000\n#\n# The amount of time unacknowledged data is allowed on a connection before we throw out the connection\n# Note this is only supported on Linux + epoll, and it appears to behave oddly above a setting of 30000\n# (it takes much longer than 30s) as of Linux 4.12. If you want something that high set this to 0\n# which picks up the OS default and configure the net.ipv4.tcp_retries2 sysctl to be ~8.\n# internode_tcp_user_timeout_in_ms: 30000\n\n# The amount of time unacknowledged data is allowed on a streaming connection.\n# The default is 5 minutes. Increase it or set it to 0 in order to increase the timeout.\n# internode_streaming_tcp_user_timeout_in_ms: 300000\n\n# Global, per-endpoint and per-connection limits imposed on messages queued for delivery to other nodes\n# and waiting to be processed on arrival from other nodes in the cluster.  These limits are applied to the on-wire\n# size of the message being sent or received.\n#\n# The basic per-link limit is consumed in isolation before any endpoint or global limit is imposed.\n# Each node-pair has three links: urgent, small and large.  So any given node may have a maximum of\n# N*3*(internode_application_send_queue_capacity_in_bytes+internode_application_receive_queue_capacity_in_bytes)\n# messages queued without any coordination between them although in practice, with token-aware routing, only RF*tokens\n# nodes should need to communicate with significant bandwidth.\n#\n# The per-endpoint limit is imposed on all messages exceeding the per-link limit, simultaneously with the global limit,\n# on all links to or from a single node in the cluster.\n# The global limit is imposed on all messages exceeding the per-link limit, simultaneously with the per-endpoint limit,\n# on all links to or from any node in the cluster.\n#\n# internode_application_send_queue_capacity_in_bytes: 4194304                       #4MiB\n# internode_application_send_queue_reserve_endpoint_capacity_in_bytes: 134217728    #128MiB\n# internode_application_send_queue_reserve_global_capacity_in_bytes: 536870912      #512MiB\n# internode_application_receive_queue_capacity_in_bytes: 4194304                    #4MiB\n# internode_application_receive_queue_reserve_endpoint_capacity_in_bytes: 134217728 #128MiB\n# internode_application_receive_queue_reserve_global_capacity_in_bytes: 536870912   #512MiB\n\n\n# How long before a node logs slow queries. Select queries that take longer than\n# this timeout to execute, will generate an aggregated log message, so that slow queries\n# can be identified. Set this value to zero to disable slow query logging.\nslow_query_log_timeout_in_ms: 500\n\n# Enable operation timeout information exchange between nodes to accurately\n# measure request timeouts.  If disabled, replicas will assume that requests\n# were forwarded to them instantly by the coordinator, which means that\n# under overload conditions we will waste that much extra time processing \n# already-timed-out requests.\n#\n# Warning: It is generally assumed that users have setup NTP on their clusters, and that clocks are modestly in sync, \n# since this is a requirement for general correctness of last write wins.\n#cross_node_timeout: true\n\n# Set keep-alive period for streaming\n# This node will send a keep-alive message periodically with this period.\n# If the node does not receive a keep-alive message from the peer for\n# 2 keep-alive cycles the stream session times out and fail\n# Default value is 300s (5 minutes), which means stalled stream\n# times out in 10 minutes by default\n# streaming_keep_alive_period_in_secs: 300\n\n# Limit number of connections per host for streaming\n# Increase this when you notice that joins are CPU-bound rather that network\n# bound (for example a few nodes with big files).\n# streaming_connections_per_host: 1\n\n\n# phi value that must be reached for a host to be marked down.\n# most users should never need to adjust this.\n# phi_convict_threshold: 8\n\n# endpoint_snitch -- Set this to a class that implements\n# IEndpointSnitch.  The snitch has two functions:\n#\n# - it teaches Cassandra enough about your network topology to route\n#   requests efficiently\n# - it allows Cassandra to spread replicas around your cluster to avoid\n#   correlated failures. It does this by grouping machines into\n#   "datacenters" and "racks."  Cassandra will do its best not to have\n#   more than one replica on the same "rack" (which may not actually\n#   be a physical location)\n#\n# CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH\n# ONCE DATA IS INSERTED INTO THE CLUSTER.  This would cause data loss.\n# This means that if you start with the default SimpleSnitch, which\n# locates every node on "rack1" in "datacenter1", your only options\n# if you need to add another datacenter are GossipingPropertyFileSnitch\n# (and the older PFS).  From there, if you want to migrate to an\n# incompatible snitch like Ec2Snitch you can do it by adding new nodes\n# under Ec2Snitch (which will locate them in a new "datacenter") and\n# decommissioning the old ones.\n#\n# Out of the box, Cassandra provides:\n#\n# SimpleSnitch:\n#    Treats Strategy order as proximity. This can improve cache\n#    locality when disabling read repair.  Only appropriate for\n#    single-datacenter deployments.\n#\n# GossipingPropertyFileSnitch\n#    This should be your go-to snitch for production use.  The rack\n#    and datacenter for the local node are defined in\n#    cassandra-rackdc.properties and propagated to other nodes via\n#    gossip.  If cassandra-topology.properties exists, it is used as a\n#    fallback, allowing migration from the PropertyFileSnitch.\n#\n# PropertyFileSnitch:\n#    Proximity is determined by rack and data center, which are\n#    explicitly configured in cassandra-topology.properties.\n#\n# Ec2Snitch:\n#    Appropriate for EC2 deployments in a single Region. Loads Region\n#    and Availability Zone information from the EC2 API. The Region is\n#    treated as the datacenter, and the Availability Zone as the rack.\n#    Only private IPs are used, so this will not work across multiple\n#    Regions.\n#\n# Ec2MultiRegionSnitch:\n#    Uses public IPs as broadcast_address to allow cross-region\n#    connectivity.  (Thus, you should set seed addresses to the public\n#    IP as well.) You will need to open the storage_port or\n#    ssl_storage_port on the public IP firewall.  (For intra-Region\n#    traffic, Cassandra will switch to the private IP after\n#    establishing a connection.)\n#\n# RackInferringSnitch:\n#    Proximity is determined by rack and data center, which are\n#    assumed to correspond to the 3rd and 2nd octet of each node\'s IP\n#    address, respectively.  Unless this happens to match your\n#    deployment conventions, this is best used as an example of\n#    writing a custom Snitch class and is provided in that spirit.\n#\n# You can use a custom Snitch by setting this to the full class name\n# of the snitch, which will be assumed to be on your classpath.\nendpoint_snitch: SimpleSnitch\n\n# controls how often to perform the more expensive part of host score\n# calculation\ndynamic_snitch_update_interval_in_ms: 100 \n# controls how often to reset all host scores, allowing a bad host to\n# possibly recover\ndynamic_snitch_reset_interval_in_ms: 600000\n# if set greater than zero, this will allow\n# \'pinning\' of replicas to hosts in order to increase cache capacity.\n# The badness threshold will control how much worse the pinned host has to be\n# before the dynamic snitch will prefer other replicas over it.  This is\n# expressed as a double which represents a percentage.  Thus, a value of\n# 0.2 means Cassandra would continue to prefer the static snitch values\n# until the pinned host was 20% worse than the fastest.\ndynamic_snitch_badness_threshold: 1.0\n\n# Configure server-to-server internode encryption\n#\n# JVM and netty defaults for supported SSL socket protocols and cipher suites can\n# be replaced using custom encryption options. This is not recommended\n# unless you have policies in place that dictate certain settings, or\n# need to disable vulnerable ciphers or protocols in case the JVM cannot\n# be updated.\n#\n# FIPS compliant settings can be configured at JVM level and should not\n# involve changing encryption settings here:\n# https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\n#\n# **NOTE** this default configuration is an insecure configuration. If you need to\n# enable server-to-server encryption generate server keystores (and truststores for mutual\n# authentication) per:\n# http://download.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n# Then perform the following configuration changes:\n#\n# Step 1: Set internode_encryption=<dc|rack|all> and explicitly set optional=true. Restart all nodes\n#\n# Step 2: Set optional=false (or remove it) and if you generated truststores and want to use mutual\n# auth set require_client_auth=true. Restart all nodes\nserver_encryption_options:\n    # On outbound connections, determine which type of peers to securely connect to.\n    #   The available options are :\n    #     none : Do not encrypt outgoing connections\n    #     dc   : Encrypt connections to peers in other datacenters but not within datacenters\n    #     rack : Encrypt connections to peers in other racks but not within racks\n    #     all  : Always use encrypted connections\n    internode_encryption: none\n    # When set to true, encrypted and unencrypted connections are allowed on the storage_port\n    # This should _only be true_ while in unencrypted or transitional operation\n    # optional defaults to true if internode_encryption is none\n    # optional: true\n    # If enabled, will open up an encrypted listening socket on ssl_storage_port. Should only be used\n    # during upgrade to 4.0; otherwise, set to false.\n    enable_legacy_ssl_storage_port: false\n    # Set to a valid keystore if internode_encryption is dc, rack or all\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # Verify peer server certificates\n    require_client_auth: false\n    # Set to a valid trustore if require_client_auth is true\n    truststore: conf/.truststore\n    truststore_password: cassandra\n    # Verify that the host name in the certificate matches the connected host\n    require_endpoint_verification: false\n    # More advanced defaults:\n    # protocol: TLS\n    # store_type: JKS\n    # cipher_suites: [\n    #   TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n    #   TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,\n    #   TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA,\n    #   TLS_RSA_WITH_AES_256_CBC_SHA\n    # ]\n\n# Configure client-to-server encryption.\n#\n# **NOTE** this default configuration is an insecure configuration. If you need to\n# enable client-to-server encryption generate server keystores (and truststores for mutual\n# authentication) per:\n# http://download.oracle.com/javase/8/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\n# Then perform the following configuration changes:\n#\n# Step 1: Set enabled=true and explicitly set optional=true. Restart all nodes\n#\n# Step 2: Set optional=false (or remove it) and if you generated truststores and want to use mutual\n# auth set require_client_auth=true. Restart all nodes\nclient_encryption_options:\n    # Enable client-to-server encryption\n    enabled: false\n    # When set to true, encrypted and unencrypted connections are allowed on the native_transport_port\n    # This should _only be true_ while in unencrypted or transitional operation\n    # optional defaults to true when enabled is false, and false when enabled is true.\n    # optional: true\n    # Set keystore and keystore_password to valid keystores if enabled is true\n    keystore: conf/.keystore\n    keystore_password: cassandra\n    # Verify client certificates\n    require_client_auth: false\n    # Set trustore and truststore_password if require_client_auth is true\n    # truststore: conf/.truststore\n    # truststore_password: cassandra\n    # More advanced defaults:\n    # protocol: TLS\n    # store_type: JKS\n    # cipher_suites: [\n    #   TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,\n    #   TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,\n    #   TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA,\n    #   TLS_RSA_WITH_AES_256_CBC_SHA\n    # ]\n\n# internode_compression controls whether traffic between nodes is\n# compressed.\n# Can be:\n#\n# all\n#   all traffic is compressed\n#\n# dc\n#   traffic between different datacenters is compressed\n#\n# none\n#   nothing is compressed.\ninternode_compression: dc\n\n# Enable or disable tcp_nodelay for inter-dc communication.\n# Disabling it will result in larger (but fewer) network packets being sent,\n# reducing overhead from the TCP protocol itself, at the cost of increasing\n# latency if you block for cross-datacenter responses.\ninter_dc_tcp_nodelay: false\n\n# TTL for different trace types used during logging of the repair process.\ntracetype_query_ttl: 86400\ntracetype_repair_ttl: 604800\n\n# If unset, all GC Pauses greater than gc_log_threshold_in_ms will log at\n# INFO level\n# UDFs (user defined functions) are disabled by default.\n# As of Cassandra 3.0 there is a sandbox in place that should prevent execution of evil code.\nenable_user_defined_functions: false\n\n# Enables scripted UDFs (JavaScript UDFs).\n# Java UDFs are always enabled, if enable_user_defined_functions is true.\n# Enable this option to be able to use UDFs with "language javascript" or any custom JSR-223 provider.\n# This option has no effect, if enable_user_defined_functions is false.\nenable_scripted_user_defined_functions: false\n\n# The default Windows kernel timer and scheduling resolution is 15.6ms for power conservation.\n# Lowering this value on Windows can provide much tighter latency and better throughput, however\n# some virtualized environments may see a negative performance impact from changing this setting\n# below their system default. The sysinternals \'clockres\' tool can confirm your system\'s default\n# setting.\nwindows_timer_interval: 1\n\n\n# Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\n# a JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\n# the "key_alias" is the only key that will be used for encrypt opertaions; previously used keys\n# can still (and should!) be in the keystore and will be used on decrypt operations\n# (to handle the case of key rotation).\n#\n# It is strongly recommended to download and install Java Cryptography Extension (JCE)\n# Unlimited Strength Jurisdiction Policy Files for your version of the JDK.\n# (current link: http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html)\n#\n# Currently, only the following file types are supported for transparent data encryption, although\n# more are coming in future cassandra releases: commitlog, hints\ntransparent_data_encryption_options:\n    enabled: false\n    chunk_length_kb: 64\n    cipher: AES/CBC/PKCS5Padding\n    key_alias: testing:1\n    # CBC IV length for AES needs to be 16 bytes (which is also the default size)\n    # iv_length: 16\n    key_provider:\n      - class_name: org.apache.cassandra.security.JKSKeyProvider\n        parameters:\n          - keystore: conf/.keystore\n            keystore_password: cassandra\n            store_type: JCEKS\n            key_password: cassandra\n\n\n#####################\n# SAFETY THRESHOLDS #\n#####################\n\n# When executing a scan, within or across a partition, we need to keep the\n# tombstones seen in memory so we can return them to the coordinator, which\n# will use them to make sure other replicas also know about the deleted rows.\n# With workloads that generate a lot of tombstones, this can cause performance\n# problems and even exaust the server heap.\n# (http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets)\n# Adjust the thresholds here if you understand the dangers and want to\n# scan more tombstones anyway.  These thresholds may also be adjusted at runtime\n# using the StorageService mbean.\ntombstone_warn_threshold: 1000\ntombstone_failure_threshold: 100000\n\n# Filtering and secondary index queries at read consistency levels above ONE/LOCAL_ONE use a\n# mechanism called replica filtering protection to ensure that results from stale replicas do\n# not violate consistency. (See CASSANDRA-8272 and CASSANDRA-15907 for more details.) This\n# mechanism materializes replica results by partition on-heap at the coordinator. The more possibly\n# stale results returned by the replicas, the more rows materialized during the query.\nreplica_filtering_protection:\n    # These thresholds exist to limit the damage severely out-of-date replicas can cause during these\n    # queries. They limit the number of rows from all replicas individual index and filtering queries\n    # can materialize on-heap to return correct results at the desired read consistency level.\n    #\n    # "cached_replica_rows_warn_threshold" is the per-query threshold at which a warning will be logged.\n    # "cached_replica_rows_fail_threshold" is the per-query threshold at which the query will fail.\n    #\n    # These thresholds may also be adjusted at runtime using the StorageService mbean.\n    #\n    # If the failure threshold is breached, it is likely that either the current page/fetch size\n    # is too large or one or more replicas is severely out-of-sync and in need of repair.\n    cached_rows_warn_threshold: 2000\n    cached_rows_fail_threshold: 32000\n\n# Log WARN on any multiple-partition batch size exceeding this value. 5kb per batch by default.\n# Caution should be taken on increasing the size of this threshold as it can lead to node instability.\nbatch_size_warn_threshold_in_kb: 5\n\n# Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default.\nbatch_size_fail_threshold_in_kb: 50\n\n# Log WARN on any batches not of type LOGGED than span across more partitions than this limit\nunlogged_batch_across_partitions_warn_threshold: 10\n\n# Log a warning when compacting partitions larger than this value\ncompaction_large_partition_warning_threshold_mb: 100\n\n# GC Pauses greater than 200 ms will be logged at INFO level\n# This threshold can be adjusted to minimize logging if necessary\n# gc_log_threshold_in_ms: 200\n\n# GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\n# Adjust the threshold based on your application throughput requirement. Setting to 0\n# will deactivate the feature.\n# gc_warn_threshold_in_ms: 1000\n\n# Maximum size of any value in SSTables. Safety measure to detect SSTable corruption\n# early. Any value size larger than this threshold will result into marking an SSTable\n# as corrupted. This should be positive and less than 2048.\n# max_value_size_in_mb: 256\n\n# Track a metric per keyspace indicating whether replication achieved the ideal consistency\n# level for writes without timing out. This is different from the consistency level requested by\n# each write which may be lower in order to facilitate availability.\n# ideal_consistency_level: EACH_QUORUM\n\n# Automatically upgrade sstables after upgrade - if there is no ordinary compaction to do, the\n# oldest non-upgraded sstable will get upgraded to the latest version\n# automatic_sstable_upgrade: false\n# Limit the number of concurrent sstable upgrades\n# max_concurrent_automatic_sstable_upgrades: 1\n\n# Audit logging - Logs every incoming CQL command request, authentication to a node. See the docs\n# on audit_logging for full details about the various configuration options.\naudit_logging_options:\n    enabled: false\n    logger:\n      - class_name: BinAuditLogger\n    # audit_logs_dir:\n    # included_keyspaces:\n    # excluded_keyspaces: system, system_schema, system_virtual_schema\n    # included_categories:\n    # excluded_categories:\n    # included_users:\n    # excluded_users:\n    # roll_cycle: HOURLY\n    # block: true\n    # max_queue_weight: 268435456 # 256 MiB\n    # max_log_size: 17179869184 # 16 GiB\n    ## archive command is "/path/to/script.sh %path" where %path is replaced with the file being rolled:\n    # archive_command:\n    # max_archive_retries: 10\n\n\n# default options for full query logging - these can be overridden from command line when executing\n# nodetool enablefullquerylog\n#full_query_logging_options:\n    # log_dir:\n    # roll_cycle: HOURLY\n    # block: true\n    # max_queue_weight: 268435456 # 256 MiB\n    # max_log_size: 17179869184 # 16 GiB\n    ## archive command is "/path/to/script.sh %path" where %path is replaced with the file being rolled:\n    # archive_command:\n    # max_archive_retries: 10\n\n# validate tombstones on reads and compaction\n# can be either "disabled", "warn" or "exception"\n# corrupted_tombstone_strategy: disabled\n\n# Diagnostic Events #\n# If enabled, diagnostic events can be helpful for troubleshooting operational issues. Emitted events contain details\n# on internal state and temporal relationships across events, accessible by clients via JMX.\ndiagnostic_events_enabled: false\n\n# Use native transport TCP message coalescing. If on upgrade to 4.0 you found your throughput decreasing, and in\n# particular you run an old kernel or have very fewer client connections, this option might be worth evaluating.\n#native_transport_flush_in_batches_legacy: false\n\n# Enable tracking of repaired state of data during reads and comparison between replicas\n# Mismatches between the repaired sets of replicas can be characterized as either confirmed\n# or unconfirmed. In this context, unconfirmed indicates that the presence of pending repair\n# sessions, unrepaired partition tombstones, or some other condition means that the disparity\n# cannot be considered conclusive. Confirmed mismatches should be a trigger for investigation\n# as they may be indicative of corruption or data loss.\n# There are separate flags for range vs partition reads as single partition reads are only tracked\n# when CL > 1 and a digest mismatch occurs. Currently, range queries don\'t use digests so if\n# enabled for range reads, all range reads will include repaired data tracking. As this adds\n# some overhead, operators may wish to disable it whilst still enabling it for partition reads\nrepaired_data_tracking_for_range_reads_enabled: false\nrepaired_data_tracking_for_partition_reads_enabled: false\n# If false, only confirmed mismatches will be reported. If true, a separate metric for unconfirmed\n# mismatches will also be recorded. This is to avoid potential signal:noise issues are unconfirmed\n# mismatches are less actionable than confirmed ones.\nreport_unconfirmed_repaired_data_mismatches: false\n\n# Having many tables and/or keyspaces negatively affects performance of many operations in the\n# cluster. When the number of tables/keyspaces in the cluster exceeds the following thresholds\n# a client warning will be sent back to the user when creating a table or keyspace.\n# table_count_warn_threshold: 150\n# keyspace_count_warn_threshold: 40\n\n#########################\n# EXPERIMENTAL FEATURES #\n#########################\n\n# Enables materialized view creation on this node.\n# Materialized views are considered experimental and are not recommended for production use.\nenable_materialized_views: false\n\n# Enables SASI index creation on this node.\n# SASI indexes are considered experimental and are not recommended for production use.\nenable_sasi_indexes: false\n\n# Enables creation of transiently replicated keyspaces on this node.\n# Transient replication is experimental and is not recommended for production use.\nenable_transient_replication: false\n\n# Enables the used of \'ALTER ... DROP COMPACT STORAGE\' statements on this node.\n# \'ALTER ... DROP COMPACT STORAGE\' is considered experimental and is not recommended for production use.\nenable_drop_compact_storage: false\n'
    with open("/etc/cassandra/cassandra.yaml", "w") as cassyconf:
        cassyconf.write(content)

if __name__ == "__main__":
    main()